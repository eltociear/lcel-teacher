{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1759959-da53-42f4-9600-45cc35b7785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "% pip install weaviate-client openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8776f494-a6fd-42bf-86df-c81bdd36794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_langchain import final_answer_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1242e55d-a0a0-4abf-8689-31abf2d8e2c8",
   "metadata": {},
   "source": [
    "## Check\n",
    "\n",
    "Invoke the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac5f79d-9592-4f5d-a308-18a7cf932ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"how to chat PDF with chroma? Use LCEL\"\n",
    "answer = final_answer_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9f71f19-27fa-468b-8349-41ca0c98d5d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is a combined code snippet that demonstrates how to chat with a PDF using Chroma and LangChain Expression Language (LCEL):\\n\\n```python\\nfrom langchain.document_loaders import PyPDFLoader\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\\nfrom langchain.chains import RetrievalQA\\nfrom langchain.llms import OpenAI\\nfrom langchain.embeddings import YourEmbeddingFunction\\n\\n# Step 1: Load PDF content\\nloader = PyPDFLoader(\"path/to/pdf/file.pdf\")\\npages = loader.load_and_split()\\n\\n# Step 2: Create an embedding function\\nembedding_function = YourEmbeddingFunction()\\n\\n# Step 3: Load PDF pages into Chroma\\nvectorstore = Chroma.from_documents(pages, embedding_function)\\n\\n# Step 4: Create a SelfQueryRetriever\\nretriever = SelfQueryRetriever.from_vectorstore(vectorstore)\\n\\n# Step 5: Define the question you want to query\\nquestion = \"Your question here\"\\n\\n# Step 6: Use LCEL to construct the query expression\\nquery_expression = f\"retrieve({{question: \\'{question}\\'}})\"\\n\\n# Step 7: Execute the query and retrieve the relevant documents\\nqa_chain = RetrievalQA(llm=OpenAI(), retriever=retriever)\\nresult = qa_chain(query_expression)\\n\\n# Step 8: Extract the answer from the result\\nanswer = result[\"answer\"]\\n\\n# Print the answer\\nprint(answer)\\n```\\n\\nIn this code snippet, we first load the PDF content and split it into pages. Then, we create an embedding function and load the PDF pages into Chroma. After that, we create a SelfQueryRetriever using the Chroma VectorStore. We define the question we want to query and use LCEL to construct the query expression. Finally, we execute the query and retrieve the relevant documents, extract the answer from the result, and print the answer.\\n\\nPlease note that you need to replace `YourEmbeddingFunction` with the actual embedding function you are using, and replace `\"Your question here\"` with the actual question you want to query.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a17c67-85fa-4da9-9ec9-4e15c9b6d8e9",
   "metadata": {},
   "source": [
    "# Eval\n",
    "\n",
    "Run eval, using `eval.csv` to generate a dataset `code-langchain-eval`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6929f3-9fd5-4dee-a16c-7b3facc557b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain.smith import RunEvalConfig\n",
    "from langsmith.client import Client\n",
    "\n",
    "# Config\n",
    "client = Client()\n",
    "eval_config = RunEvalConfig(\n",
    "    evaluators=[\"cot_qa\"],\n",
    ")\n",
    "\n",
    "# Experiments\n",
    "chain_map = {\n",
    "    \"code-langchain-v1\": final_answer_chain,\n",
    "}\n",
    "\n",
    "# Run evaluation\n",
    "run_id = uuid.uuid4().hex[:4]\n",
    "test_runs = {}\n",
    "for project_name, chain in chain_map.items():\n",
    "    test_runs[project_name] = client.run_on_dataset(\n",
    "        dataset_name=\"code-langchain-eval\",\n",
    "        llm_or_chain_factory=chain,\n",
    "        evaluation=eval_config,\n",
    "        verbose=True,\n",
    "        project_name=f\"{run_id}-{project_name}\",\n",
    "        project_metadata={\"chain\": project_name},\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
