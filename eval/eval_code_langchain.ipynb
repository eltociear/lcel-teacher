{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8776f494-a6fd-42bf-86df-c81bdd36794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')  \n",
    "from app.chain import chain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1242e55d-a0a0-4abf-8689-31abf2d8e2c8",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9f71f19-27fa-468b-8349-41ca0c98d5d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is a combined code snippet that demonstrates how to chat with a PDF using Chroma and LCEL in LangChain:\\n\\n```python\\n# Import necessary modules\\nfrom langchain.document_loaders import TextLoader\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\\nfrom langchain.vectorstores.chroma import Chroma\\nfrom langchain.chains.query_constructor.schema import AttributeInfo\\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\\n\\n# Load the PDF\\nloader = TextLoader()\\ndocument = loader.load(\\'path_to_your_pdf.pdf\\')\\n\\n# Split the document\\nsplitter = CharacterTextSplitter()\\nchunks = splitter.split(document)\\n\\n# Create an embedding function\\nembedder = SentenceTransformerEmbeddings(\\'all-MiniLM-L6-v2\\')\\nembeddings = embedder.embed(chunks)\\n\\n# Create a Chroma database\\nvectorstore = Chroma.from_documents(chunks, embeddings)\\n\\n# Define the metadata field information\\nmetadata_field_info = [\\n    AttributeInfo(\\n        name=\"page_number\",\\n        description=\"The page number of the chunk in the PDF\",\\n        type=\"integer\",\\n    ),\\n    # Add more metadata fields as needed\\n]\\n\\n# Create a self-querying retriever\\nretriever = SelfQueryRetriever.from_llm(\\n    llm, vectorstore, document_content_description, metadata_field_info, verbose=True\\n)\\n\\n# Use LCEL to query the vector database\\nquery = \"page_number:1\"\\nresults = retriever.get_relevant_documents(query)\\n\\n# Display the results\\nfor result in results:\\n    print(result.content)\\n```\\n\\nThis code snippet loads a PDF, splits it into chunks, creates vector embeddings for the chunks, stores the embeddings in a Chroma database, and uses LCEL to query the database. The results are then printed to the console. Please replace `\\'path_to_your_pdf.pdf\\'` with the actual path to your PDF file and `\\'all-MiniLM-L6-v2\\'` with the actual model name you want to use for embeddings. Also, replace `llm` and `document_content_description` with your actual large language model and document content description, respectively.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"how to chat PDF with chroma? Use LCEL\"\n",
    "answer = chain.invoke({\"question\": question})\n",
    "answer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72a17c67-85fa-4da9-9ec9-4e15c9b6d8e9",
   "metadata": {},
   "source": [
    "# Eval\n",
    "\n",
    "Run eval, using `eval.csv` to generate a dataset `code-langchain-eval`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6929f3-9fd5-4dee-a16c-7b3facc557b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain.smith import RunEvalConfig\n",
    "from langsmith.client import Client\n",
    "\n",
    "# Config\n",
    "client = Client()\n",
    "eval_config = RunEvalConfig(\n",
    "    evaluators=[\"cot_qa\"],\n",
    ")\n",
    "\n",
    "# Experiments\n",
    "chain_map = {\n",
    "    \"code-langchain-v1\": chain,\n",
    "}\n",
    "\n",
    "# Run evaluation\n",
    "run_id = uuid.uuid4().hex[:4]\n",
    "test_runs = {}\n",
    "for project_name, chain in chain_map.items():\n",
    "    test_runs[project_name] = client.run_on_dataset(\n",
    "        dataset_name=\"code-langchain-eval\",\n",
    "        llm_or_chain_factory=chain,\n",
    "        evaluation=eval_config,\n",
    "        verbose=True,\n",
    "        project_name=f\"{run_id}-{project_name}\",\n",
    "        project_metadata={\"chain\": project_name},\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
