Question,Answer
"How do I set up a retrieval-augmented generation chain using LCEL?","Here is an example RAG chain using LCEL: \n\nfrom operator import itemgetter\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.vectorstores import FAISS\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnableLambda, RunnablePassthrough\n\nvectorstore = FAISS.from_texts(\n    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings()\n)\nretriever = vectorstore.as_retriever()\n\ntemplate = \"\"\"Answer the question based only on the following context:\n{context}\n\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\n\nmodel = ChatOpenAI()\n\nchain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | model\n    | StrOutputParser()\n)\n\nchain.invoke(\"where did harrison work?\")"
"How can I use a prompt and model to create a chain in LCEL?","Here is an example of a prompt + LLM chain using LCEL: \nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\n\nprompt = ChatPromptTemplate.from_template(\"tell me a joke about {foo}\")\nmodel = ChatOpenAI()\nchain = prompt | model\n\nchain.invoke({\"foo\": \"bears\"})"
"How can I add memory to an arbitrary chain using LCEL?","Here is an example adding memory to a chain: \nfrom operator import itemgetter\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables import RunnableLambda, RunnablePassthrough\n\nmodel = ChatOpenAI()\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are a helpful chatbot\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{input}\"),\n    ]\n)\n\nmemory = ConversationBufferMemory(return_messages=True)\n\nchain = (\n    RunnablePassthrough.assign(\n        history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n    )\n    | prompt\n    | model\n)\n\ninputs = {\"input\": \"hi im bob\"}\nresponse = chain.invoke(inputs)"