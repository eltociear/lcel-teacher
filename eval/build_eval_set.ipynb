{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc4d2f37-d4af-4076-a14f-a004af5fc7da",
   "metadata": {},
   "source": [
    "## Get LCEL-related questions from `chat-langchain`\n",
    "\n",
    "See [here](https://raw.githubusercontent.com/hinthornw/lspopscripts/main/download_runs.py) if you want code the get full traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a20d3f7e-a435-4816-a5bc-36ae4c1a570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"ls__844575bca0324625ae10cd97bb9c2888\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2adea52-baea-4634-a692-f49a1df571c7",
   "metadata": {},
   "source": [
    "## Get Questions\n",
    "\n",
    "* Extract from `chat-langchain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "052a5d8c-23cb-47ea-8a52-9c030c3c6a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "152090it [42:48, 59.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to fetched_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import csv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from itertools import islice\n",
    "\n",
    "import langsmith\n",
    "from tqdm import tqdm\n",
    "\n",
    "client = langsmith.Client()\n",
    "\n",
    "def download_data(\n",
    "    project_name: str,\n",
    "    nested: bool = False,\n",
    "    since: datetime.datetime = yesterday,\n",
    "    exclude_followups: bool = True,\n",
    "    filename: str = \"fetched_data.csv\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Downloads and saves data from Langsmith runs to a CSV file.\n",
    "\n",
    "    This function retrieves run data from the Langsmith project specified by 'project_name'.\n",
    "    It extracts 'question' and 'output' from each run's inputs and outputs, respectively,\n",
    "    and saves them into a CSV file. The function can handle both nested and non-nested runs.\n",
    "    Follow-up runs can be excluded if desired.\n",
    "\n",
    "    Parameters:\n",
    "    project_name (str): The name of the Langsmith project to retrieve data from.\n",
    "    nested (bool): Set to True to handle nested runs; False by default.\n",
    "    since (datetime): The start time from which to retrieve runs; defaults to yesterday.\n",
    "    exclude_followups (bool): Set to True to exclude follow-up runs; True by default.\n",
    "    filename (str): The name of the file to save the data to; defaults to 'fetched_data.csv'.\n",
    "    \"\"\"\n",
    "    traces = client.list_runs(\n",
    "        project_name=project_name, start_time=since, execution_order=1\n",
    "    )\n",
    "    batch_size = 10\n",
    "    executor = ThreadPoolExecutor(max_workers=batch_size) if nested else None\n",
    "\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file_handle:\n",
    "        csv_writer = csv.writer(file_handle)\n",
    "        # Write the header\n",
    "        csv_writer.writerow(['question', 'output'])\n",
    "\n",
    "        try:\n",
    "            if nested:\n",
    "                pbar = tqdm()\n",
    "                while True:\n",
    "                    batch = list(islice(traces, batch_size))\n",
    "                    if not batch:\n",
    "                        break\n",
    "                    futures = [\n",
    "                        executor.submit(client.read_run, run.id, load_child_runs=True)\n",
    "                        for run in batch\n",
    "                    ]\n",
    "                    for future in as_completed(futures):\n",
    "                        loaded_run = future.result()\n",
    "                        loaded_run_json=loaded_run.json()\n",
    "                        loaded_run_json = json.loads(loaded_run_json)\n",
    "                        question = loaded_run_json['inputs'].get('question', '')\n",
    "                        output = loaded_run_json['outputs'].get('output', '')\n",
    "                        csv_writer.writerow([question, output])\n",
    "                    pbar.update(len(batch))\n",
    "            else:\n",
    "                for run in tqdm(traces):\n",
    "                    if exclude_followups and run.inputs.get(\"chat_history\"):\n",
    "                        continue\n",
    "                    run_json = run.json()\n",
    "                    run_json = json.loads(run_json)\n",
    "                    question = run_json['inputs'].get('question', '')\n",
    "                    output = run_json['outputs'].get('output', '')\n",
    "                    csv_writer.writerow([question, output])\n",
    "\n",
    "        finally:\n",
    "            if executor:\n",
    "                executor.shutdown()\n",
    "    \n",
    "    print(f\"Saved to {filename}\")\n",
    "\n",
    "# Call the function\n",
    "yesterday = datetime.datetime.now() - datetime.timedelta(days=1)\n",
    "window_30_day = datetime.datetime.now() - datetime.timedelta(days=30)\n",
    "download_data(project_name=\"chat-langchain\",\n",
    "              since=window_30_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9592ccc-5776-4e1d-b062-f3264929e023",
   "metadata": {},
   "source": [
    "## Read Extracted QA Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3406211-321a-40ff-8c9d-9ba48d83da23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(447, 2)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'fetched_data.csv' with the path to your CSV file\n",
    "filename = 'fetched_data.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "search_term = 'LCEL'\n",
    "filtered_df = df[df['question'].str.contains(search_term, case=False, na=False)]\n",
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b583a444-c256-4466-879f-2de8b9bbd972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63797, 2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0863bb39-759a-49f5-8668-5daf544c38a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2126.5666666666666"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "63797 / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7b282e0-e114-4856-8924-cc62f075d5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I know that there is QA.chain in langchahin, w...</td>\n",
       "      <td>In addition to the `QA.chain` in Langchain, th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  I know that there is QA.chain in langchahin, w...   \n",
       "\n",
       "                                              output  \n",
       "0                                                NaN  \n",
       "1                                                NaN  \n",
       "2  In addition to the `QA.chain` in Langchain, th...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f2e2d3-b2b3-45cd-a253-c48e19936ff7",
   "metadata": {},
   "source": [
    "## Extract Questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93d4415c-e916-4b5c-9fd0-8c86a034675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Group by unique instances of 'question' and then reset index\n",
    "unique_questions_df = filtered_df.drop_duplicates(subset='question')\n",
    "\n",
    "# Extract the 'question' column and convert it to a list\n",
    "unique_questions = unique_questions_df['question'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f037e6e0-6f04-41a1-971f-7843bcb36d51",
   "metadata": {},
   "source": [
    "## Cluster\n",
    "\n",
    "Some of the questions are highly verbose and contain large code blocks.\n",
    "\n",
    "Let's try to cluster so that they are at least grouped when we summarize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3a67c4-c33e-42d4-9be7-25694e2b345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed and cluster \n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embd = OpenAIEmbeddings()\n",
    "question_embeddings = embd.embed_documents(unique_questions)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "clustering_model = KMeans(n_clusters=5, random_state=0)\n",
    "clusters = clustering_model.fit_predict(question_embeddings)\n",
    "unique_questions_df['cluster'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2dabff7-3272-4143-af54-141f798cf00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format\n",
    "\n",
    "def fmt_qus(df):\n",
    "\n",
    "    unique_questions = df['question'].tolist()\n",
    "    formatted_unique_questions = '--- --- \\n --- --- '.join(unique_questions)\n",
    "    return formatted_unique_questions\n",
    "\n",
    "# Get unique values in the 'cluster' column\n",
    "all_clusters = unique_questions_df['cluster'].unique()\n",
    "\n",
    "# Process each cluster\n",
    "cluster_context=[]\n",
    "for i in all_clusters:\n",
    "    df_cluster = unique_questions_df[unique_questions_df['cluster'] == i]\n",
    "    formatted_questions = fmt_qus(df_cluster)\n",
    "    cluster_context.append(formatted_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0884c6-1b3d-49f2-b247-f7e9854b598d",
   "metadata": {},
   "source": [
    "## Summarize\n",
    "\n",
    "Summarize major question themes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c8001a4c-c697-4f85-97fc-85c459d7a4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rlm/Desktop/Code/code-langchain/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:115: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use langchain_openai.ChatOpenAI instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt template\n",
    "template = \"\"\"Here is a set of questions input to LangChain QA system. \\n\n",
    "\n",
    "They are related to LCEL, LangChain Expression Language. \\n\n",
    "\n",
    "Reason about the questions, first. \\n\n",
    "\n",
    "Then, give me a list of the top 10 question themes.\n",
    "\n",
    "Give me one reprentitive question per theme.\n",
    "\n",
    "Questions:\n",
    "{context}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4-1106-preview\")\n",
    "chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "128dec15-054c-4f9b-b8bb-a3baebbd4655",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "for c in cluster_context:\n",
    "    answers.append(chain.invoke({\"context\":c}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b14a647-9d0f-4442-9963-566558c45ed8",
   "metadata": {},
   "source": [
    "## Themes\n",
    "\n",
    "* https://smith.langchain.com/public/69d0b729-cd8c-4d4b-859d-6e5ee683fc7a/r\n",
    "\n",
    "```\n",
    "1. **Basic Understanding of LCEL**\n",
    "   - What is LCEL?\n",
    "\n",
    "2. **LCEL Integration with Agents**\n",
    "   - Can I use agents with LCEL?\n",
    "\n",
    "3. **LCEL Coding and Implementation Examples**\n",
    "   - Code me a question answering example with LCEL.\n",
    "\n",
    "4. **LCEL with Memory and Storage**\n",
    "   - How to use VectorStoreRetrieverMemory in LCEL?\n",
    "\n",
    "5. **LCEL Configuration and Settings**\n",
    "   - How to set verbose true for LCEL?\n",
    "\n",
    "6. **LCEL with Retrieval-Augmented Generation (RAG)**\n",
    "   - Can you give me an example to run a simple RAG using LCEL in Python?\n",
    "\n",
    "7. **LCEL Asynchronous Operations**\n",
    "   - LCEL 异步invoke (LCEL asynchronous invoke)\n",
    "\n",
    "8. **LCEL Error Handling and Debugging**\n",
    "   - How can I get the finish reason using LCEL?\n",
    "\n",
    "9. **LCEL with Multiple Inputs and Variables**\n",
    "   - How to use multiple partial variables in LCEL?\n",
    "\n",
    "10. **LCEL Advanced Features and Customization**\n",
    "   - Can LCEL execute custom python functions?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c0186a-3336-4297-b5d7-812f4be2c5fb",
   "metadata": {},
   "source": [
    "* https://smith.langchain.com/public/b3aba7b6-e877-4d99-bede-e607138fe171/r\n",
    "\n",
    "```\n",
    "1. **Parallel and Asynchronous Execution**: Questions about running multiple chains in parallel or asynchronously.\n",
    "   - Representative question: \"I want to run three chains in parallel. They share the same input variables, but produce different output objects. How do I do this with LCEL?\"\n",
    "\n",
    "2. **Custom Functions and Configurations**: How to include custom functions or add configurable fields to a chain.\n",
    "   - Representative question: \"How to include a custom function as part of an LCEL chain?\"\n",
    "\n",
    "3. **Memory Management**: Questions about how memory is handled within chains, including buffer memory and conversation memory.\n",
    "   - Representative question: \"I have a LCEL chain with e.g. buffer memory, and I serve it via Langserve. When is the memory reset? Do all API calls use the same memory under the hood?\"\n",
    "\n",
    "4. **Chain Composition and Modularity**: How to compose chains from multiple components or steps, and how to pass data between them.\n",
    "   - Representative question: \"How can I connect several chains, i.e. the output of the former chain is the input of the latter chain? Can I achieve this through LCEL?\"\n",
    "\n",
    "5. **Error Handling and Retries**: How to handle errors and implement retries within a chain.\n",
    "   - Representative question: \"How can I use a RetryWithErrorOutputParser in a LCEL chain?\"\n",
    "\n",
    "6. **Retrieval and Querying**: Questions about setting up retrieval chains, including those with specific querying capabilities.\n",
    "   - Representative question: \"How to create a Retrieval QA chain with streaming, using LCEL?\"\n",
    "\n",
    "7. **Verbose and Debugging**: How to enable verbose output or debugging within a chain.\n",
    "   - Representative question: \"How to set verbose True in LangChain Expression Language (LCEL)?\"\n",
    "\n",
    "8. **Integration with External Services**: Questions about integrating LCEL chains with external services or databases.\n",
    "   - Representative question: \"I need a LCEL chain that takes a YouTube link and transcribes it with Whisper.\"\n",
    "\n",
    "9. **Chain Customization and Enhancement**: How to enhance chains with additional features like callbacks, custom parsers, or specific output formatting.\n",
    "   - Representative question: \"How do I pass a pre-written history variable into my LCEL chain?\"\n",
    "\n",
    "10. **Understanding LCEL Fundamentals**: Basic questions about what LCEL is and how to use it effectively.\n",
    "    - Representative question: \"What is LangChain Expression Language (LCEL)?\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec53ff1-40fc-4986-b799-bf1bdd90dbaa",
   "metadata": {},
   "source": [
    "* https://smith.langchain.com/public/3346bc94-146e-451a-9af4-7b7fe07d9a84/r\n",
    "\n",
    "```\n",
    "1. **LCEL Chain Construction**: How to build and structure chains using LCEL components.\n",
    "   - Representative question: \"Give an example of an LCEL chain with LLMSingleActionAgent and AgentExecutor.\"\n",
    "\n",
    "2. **Output Parsing and Formatting**: How to parse and format the output from LCEL chains.\n",
    "   - Representative question: \"What LangChain tool can I use to parse this output into a single message?\"\n",
    "\n",
    "3. **Component Ordering and Interaction**: Understanding the order and interaction between components in an LCEL chain.\n",
    "   - Representative question: \"When using LCEL, is the order of the chained components arbitrary?\"\n",
    "\n",
    "4. **Custom Agents and Tools Integration**: How to integrate custom agents and tools within an LCEL chain.\n",
    "   - Representative question: \"I would like to use my own custom agent in an LCEL chain. How do I build this chain?\"\n",
    "\n",
    "5. **Conditional Logic and Prompts**: Implementing conditional logic and handling prompts in LCEL.\n",
    "   - Representative question: \"How to conditionally choose between prompts in LCEL.\"\n",
    "\n",
    "6. **Memory and Conversation History**: Utilizing memory and conversation history within LCEL chains.\n",
    "   - Representative question: \"Conversation chain with memory using LCEL.\"\n",
    "\n",
    "7. **Runnable and Agent Configuration**: Configuring and using Runnables and agents in LCEL.\n",
    "   - Representative question: \"How do I configure ReAct agent 'Thought' with custom OutputParser and Custom Agent, using LCEL?\"\n",
    "\n",
    "8. **LCEL Syntax and Expressions**: Understanding and using the syntax and expressions specific to LCEL.\n",
    "   - Representative question: \"Can I create an LCEL chain with prompt templates having no input variables?\"\n",
    "\n",
    "9. **LCEL with Specific Models and Tools**: Using LCEL with specific models like GPT-4 and tools like vectorstore retrievers.\n",
    "   - Representative question: \"Can you show me an example of an agent using gpt4 with a web search tool and memory? All using LCEL.\"\n",
    "\n",
    "10. **LCEL in Different Environments and Applications**: Applying LCEL in various environments and for different types of applications.\n",
    "    - Representative question: \"Provide LCEL code for a simple chat app using Azure OpenAI.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d214d20-e3f6-4c81-a32b-c30f4f15b8b8",
   "metadata": {},
   "source": [
    "* https://smith.langchain.com/public/47648f18-b543-477c-91cf-84d612eb6810/r\n",
    "\n",
    "```\n",
    "1. **Error Handling in LCEL Chains**\n",
    "   - Representative Question: \"This code gives me this error TypeError: Expected a Runnable, callable or dict. Instead got an unsupported type: <class 'str'>\"\n",
    "\n",
    "2. **Integration of LangChain with AI Models**\n",
    "   - Representative Question: \"Create the LCEL chain using ChatOpenAI with a specific model and temperature settings.\"\n",
    "\n",
    "3. **PDF Processing with PyMuPDF**\n",
    "   - Representative Question: \"Convert a PDF page to a pixmap using the PyMuPDF library.\"\n",
    "\n",
    "4. **Base64 Encoding of Images**\n",
    "   - Representative Question: \"Encode a pixmap to a base64 string for image processing.\"\n",
    "\n",
    "5. **Template Formatting and Data Injection**\n",
    "   - Representative Question: \"Define the prompt templates and format them with dynamic data for the AI model.\"\n",
    "\n",
    "6. **AI-Assisted Data Interpretation**\n",
    "   - Representative Question: \"Use the AI model to assist in marking images using a provided mark scheme.\"\n",
    "\n",
    "7. **File I/O Operations**\n",
    "   - Representative Question: \"Write the results of the LCEL chain to a file.\"\n",
    "\n",
    "8. **Debugging Lambda Functions in LCEL**\n",
    "   - Representative Question: \"Change the RunnableLambda to RunnablePassthrough from the start of the template.\"\n",
    "\n",
    "9. **Understanding LCEL Chain Outputs**\n",
    "   - Representative Question: \"What will be the type of marking_output and how to make it a string?\"\n",
    "\n",
    "10. **Correct Usage of LCEL Components**\n",
    "    - Representative Question: \"The output of the LCEL chain isn't a string; find a way to fix it.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf30308-8aee-4e96-8020-e3f154f14f86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426f82d9-0338-49ec-92d7-d45696607b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e717e5c-7599-4b78-b5ef-8006e4e96478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef095c25-9459-4919-be21-e868fd19480a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d860136c-1880-483e-aa89-0c4bfdab3b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4685ea07-c3d1-4a88-bfe5-6fee5fdf1966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c1982b-aa57-4976-9ecb-07324306d561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccbc8bf4-dbf8-4d26-9b41-27005468cf37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac7eb600-ae3a-4f8a-b3ad-8be75e84f55d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb65e7-5f58-418f-9832-bbc42d58bff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c117a261-192c-4c75-8e07-96576f34d117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
